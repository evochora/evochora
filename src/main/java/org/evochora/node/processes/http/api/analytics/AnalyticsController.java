package org.evochora.node.processes.http.api.analytics;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import org.evochora.datapipeline.api.analytics.ManifestEntry;
import org.evochora.datapipeline.api.resources.storage.IAnalyticsStorageRead;
import org.evochora.node.processes.http.api.pipeline.dto.ErrorResponseDto;
import org.evochora.node.spi.IController;
import org.evochora.node.spi.ServiceRegistry;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.typesafe.config.Config;

import io.javalin.Javalin;
import io.javalin.http.Context;
import io.javalin.openapi.HttpMethod;
import io.javalin.openapi.OpenApi;
import io.javalin.openapi.OpenApiContent;
import io.javalin.openapi.OpenApiParam;
import io.javalin.openapi.OpenApiResponse;

/**
 * HTTP controller for serving analytics artifacts and manifest.
 * <p>
 * Provides REST API endpoints for discovering and downloading Parquet-based
 * analytics data generated by the AnalyticsIndexer.
 * <p>
 * <strong>Key features:</strong>
 * <ul>
 *   <li>Manifest aggregation (discovers all metric plugins and their metadata)</li>
 *   <li>File listing with optional prefix filter</li>
 *   <li>File streaming for Parquet, JSON, and CSV artifacts</li>
 *   <li>Response caching for manifest (configurable TTL)</li>
 * </ul>
 * <p>
 * <strong>Thread Safety:</strong> This controller is thread-safe and can handle concurrent requests.
 */
public class AnalyticsController implements IController {

    private static final Logger log = LoggerFactory.getLogger(AnalyticsController.class);
    private final IAnalyticsStorageRead storage;
    private final Gson gson = new GsonBuilder().create();
    
    // Caching for Manifest
    private final long cacheTtlMs;
    private final ConcurrentHashMap<String, CacheEntry> manifestCache = new ConcurrentHashMap<>();
    
    // DuckDB driver loaded flag (for server-side queries)
    private static volatile boolean duckDbDriverLoaded = false;

    private record CacheEntry(long timestamp, String json) {}

    /**
     * Constructs a new AnalyticsController.
     *
     * @param registry Service registry for accessing storage resources
     * @param options Controller-specific configuration
     */
    public AnalyticsController(ServiceRegistry registry, Config options) {
        this.storage = registry.get(IAnalyticsStorageRead.class);
        this.cacheTtlMs = options.hasPath("analyticsManifestCacheTtlSeconds") 
            ? options.getInt("analyticsManifestCacheTtlSeconds") * 1000L 
            : 30000L; // Default 30s
        
        // Load DuckDB driver for server-side queries
        loadDuckDbDriver();
    }
    
    /**
     * Loads the DuckDB JDBC driver (thread-safe, idempotent).
     */
    private static synchronized void loadDuckDbDriver() {
        if (!duckDbDriverLoaded) {
            try {
                Class.forName("org.duckdb.DuckDBDriver");
                duckDbDriverLoaded = true;
                log.debug("DuckDB driver loaded for server-side analytics queries");
            } catch (ClassNotFoundException e) {
                log.warn("DuckDB driver not found - /data endpoint will not work");
            }
        }
    }

    @Override
    public void registerRoutes(Javalin app, String basePath) {
        app.get(basePath + "/runs", this::listRuns);
        app.get(basePath + "/manifest", this::getManifest);
        app.get(basePath + "/data", this::queryData);
        // Merged Parquet streaming for client-side DuckDB WASM queries
        app.get(basePath + "/parquet", this::getParquet);
    }
    
    /**
     * Resolves the run ID from query parameter or falls back to latest.
     * <p>
     * If no runId parameter is provided, returns the most recent simulation run.
     * This matches the behavior of the Visualizer controllers for consistency.
     *
     * @param ctx Javalin request context
     * @return Resolved run ID
     * @throws IllegalStateException if no runs are available
     */
    private String resolveRunId(Context ctx) {
        String queryRunId = ctx.queryParam("runId");
        if (queryRunId != null && !queryRunId.trim().isEmpty()) {
            return queryRunId.trim();
        }
        
        // Fall back to latest run
        try {
            List<String> runIds = storage.listAnalyticsRunIds();
            if (runIds.isEmpty()) {
                throw new IllegalStateException("No simulation runs available");
            }
            // runIds are sorted newest first
            return runIds.get(0);
        } catch (IOException e) {
            throw new RuntimeException("Failed to find latest run ID", e);
        }
    }

    /**
     * Lists all simulation runs that have analytics data.
     * <p>
     * Route: GET /runs
     *
     * @param ctx Javalin request context
     */
    @OpenApi(
        path = "runs",
        methods = {HttpMethod.GET},
        summary = "List simulation runs with analytics",
        description = "Returns a list of all simulation run IDs that have analytics data available. "
            + "Results are sorted by timestamp (newest first).",
        tags = {"analyzer / analytics"},
        responses = {
            @OpenApiResponse(
                status = "200", 
                description = "List of run IDs with analytics data",
                content = @OpenApiContent(
                    from = RunInfo[].class,
                    example = """
                        [
                          {"runId": "20251201-143025-550e8400-e29b-41d4-a716-446655440000", "startTime": 1701437425000},
                          {"runId": "20251130-120000-abc12345-def6-7890-abcd-ef1234567890", "startTime": 1701345600000}
                        ]
                        """
                )
            ),
            @OpenApiResponse(
                status = "500", 
                description = "Internal server error", 
                content = @OpenApiContent(from = ErrorResponseDto.class)
            )
        }
    )
    private void listRuns(Context ctx) {
        try {
            List<String> runIds = storage.listAnalyticsRunIds();
            
            // Convert to RunInfo objects with parsed timestamp
            List<RunInfo> runs = runIds.stream()
                .map(runId -> {
                    Long startTime = parseRunIdTimestamp(runId);
                    return new RunInfo(runId, startTime);
                })
                .toList();
            
            ctx.json(runs);
        } catch (Exception e) {
            log.error("Failed to list analytics runs", e);
            ctx.status(500).result("Failed to list runs");
        }
    }

    /**
     * Parses timestamp from runId format: YYYYMMdd-HHmmssSS-UUID
     */
    private Long parseRunIdTimestamp(String runId) {
        if (runId == null || runId.length() < 17) {
            return null;
        }
        try {
            String timestampStr = runId.substring(0, 17); // YYYYMMdd-HHmmssSS
            java.time.format.DateTimeFormatter formatter = 
                java.time.format.DateTimeFormatter.ofPattern("yyyyMMdd-HHmmssSS");
            java.time.LocalDateTime ldt = java.time.LocalDateTime.parse(timestampStr, formatter);
            return ldt.atZone(java.time.ZoneId.systemDefault()).toInstant().toEpochMilli();
        } catch (Exception e) {
            return null;
        }
    }

    /**
     * DTO for run information.
     */
    public record RunInfo(String runId, Long startTime) {}

    /**
     * Queries analytics data and returns aggregated JSON.
     * <p>
     * Route: GET /data?runId=...&amp;metric=...&amp;lod=...
     * <p>
     * This endpoint aggregates all Parquet files for a metric/LOD into a single JSON response.
     * Uses server-side DuckDB for fast queries. Storage-backend agnostic (works with S3).
     *
     * @param ctx Javalin request context
     */
    @OpenApi(
        path = "data",
        methods = {HttpMethod.GET},
        summary = "Query analytics data",
        description = "Returns aggregated analytics data as JSON. Server-side DuckDB reads all Parquet files "
            + "and returns a single JSON array. Much faster than client-side loading of many files.",
        tags = {"analyzer / analytics"},
        queryParams = {
            @OpenApiParam(
                name = "metric",
                description = "Metric identifier (e.g., 'population')",
                required = true,
                example = "population"
            ),
            @OpenApiParam(
                name = "runId",
                description = "Simulation run ID. Optional - defaults to latest run.",
                required = false,
                example = "20251201-143025-550e8400-e29b-41d4-a716-446655440000"
            ),
            @OpenApiParam(
                name = "lod",
                description = "LOD level (e.g., 'lod0', 'lod1'). Default: auto-select based on file count.",
                required = false,
                example = "lod0"
            )
        },
        responses = {
            @OpenApiResponse(
                status = "200",
                description = "Array of data rows from all Parquet files",
                content = @OpenApiContent(
                    mimeType = "application/json",
                    example = """
                        [
                          {"tick": 0, "alive_count": 1, "total_dead": 0, "avg_energy": 10000.0},
                          {"tick": 1, "alive_count": 1, "total_dead": 0, "avg_energy": 9950.5},
                          {"tick": 2, "alive_count": 2, "total_dead": 0, "avg_energy": 9900.0}
                        ]
                        """
                )
            ),
            @OpenApiResponse(
                status = "400",
                description = "Bad request (missing parameters)",
                content = @OpenApiContent(from = ErrorResponseDto.class)
            ),
            @OpenApiResponse(
                status = "500",
                description = "Query execution failed",
                content = @OpenApiContent(from = ErrorResponseDto.class)
            )
        }
    )
    private void queryData(Context ctx) {
        String metric = ctx.queryParam("metric");
        String lod = ctx.queryParam("lod");

        if (metric == null || metric.isBlank()) {
            ctx.status(400).result("Missing metric parameter");
            return;
        }
        
        // Resolve runId (optional - defaults to latest)
        String runId;
        try {
            runId = resolveRunId(ctx);
        } catch (IllegalStateException e) {
            ctx.status(404).result("No simulation runs available");
            return;
        }

        // Resolve storage metric ID (may differ from manifest entry ID for merged plugins)
        String storageMetric = resolveStorageMetric(runId, metric);

        // Auto-select LOD if not specified (prefer higher LOD with fewer files)
        if (lod == null || lod.isBlank()) {
            lod = autoSelectLod(runId, storageMetric);
            if (lod == null) {
                ctx.status(404).result("No data found for metric: " + metric);
                return;
            }
        }

        try {
            long startTime = System.currentTimeMillis();

            // 1. List all Parquet files for this metric/LOD
            String prefix = storageMetric + "/" + lod + "/";
            List<String> files = storage.listAnalyticsFiles(runId, prefix);
            List<String> parquetFiles = files.stream()
                .filter(f -> f.endsWith(".parquet"))
                .toList();

            if (parquetFiles.isEmpty()) {
                ctx.json(List.of()); // Empty array
                return;
            }

            // 2. Copy files from storage to temp directory (storage-agnostic)
            Path tempDir = Files.createTempDirectory("analytics-query-");
            List<Path> tempFiles = new ArrayList<>();
            
            try {
                for (String file : parquetFiles) {
                    Path tempFile = tempDir.resolve(file.replace("/", "_"));
                    try (InputStream in = storage.openAnalyticsInputStream(runId, file);
                         OutputStream out = Files.newOutputStream(tempFile)) {
                        long bytesWritten = in.transferTo(out);
                        
                        // Skip empty files (incomplete from previous shutdown)
                        if (bytesWritten == 0) {
                            log.debug("Skipping empty Parquet file: {}", file);
                            Files.deleteIfExists(tempFile);
                            continue;
                        }
                    }
                    tempFiles.add(tempFile);
                }

                // 3. Query with DuckDB
                List<Map<String, Object>> result = queryParquetFiles(tempFiles);

                long duration = System.currentTimeMillis() - startTime;
                log.debug("Query {}/{}/{}: {} files, {} rows in {}ms", 
                    runId, metric, lod, parquetFiles.size(), result.size(), duration);

                // 4. Return JSON
                ctx.json(result);

            } finally {
                // 5. Cleanup temp directory recursively
                try (var paths = Files.walk(tempDir)) {
                    paths.sorted(java.util.Comparator.reverseOrder())
                         .forEach(path -> {
                    try {
                                 Files.deleteIfExists(path);
                    } catch (IOException e) {
                                 log.debug("Failed to delete: {}", path);
                    }
                         });
                } catch (IOException e) {
                    log.debug("Failed to cleanup temp directory: {}", tempDir);
                }
            }

        } catch (Exception e) {
            log.error("Query failed for {}/{}/{}", runId, metric, lod, e);
            ctx.status(500).result("Query failed: " + e.getMessage());
        }
    }

    /**
     * Auto-selects the optimal LOD level based on file count.
     * Prefers higher LOD (fewer files) if available.
     */
    private String autoSelectLod(String runId, String metricId) {
        try {
            List<String> allFiles = storage.listAnalyticsFiles(runId, metricId + "/");
            
            // Count files per LOD
            Map<String, Long> lodCounts = allFiles.stream()
                .filter(f -> f.endsWith(".parquet"))
                .map(f -> {
                    String[] parts = f.split("/");
                    return parts.length >= 2 ? parts[1] : "unknown";
                })
                .collect(java.util.stream.Collectors.groupingBy(
                    lod -> lod,
                    java.util.stream.Collectors.counting()
                ));

            if (lodCounts.isEmpty()) {
                return null;
            }

            // Select LOD with <= 50 files, or highest LOD if all have more
            // Lower threshold = faster loading (fewer files to query)
            return lodCounts.entrySet().stream()
                .sorted(Map.Entry.comparingByKey()) // lod0, lod1, lod2...
                .filter(e -> e.getValue() <= 50)
                .map(Map.Entry::getKey)
                .findFirst()
                .orElse(lodCounts.keySet().stream().max(String::compareTo).orElse("lod0"));

        } catch (Exception e) {
            log.warn("Failed to auto-select LOD for {}/{}", runId, metricId);
            return "lod0"; // Fallback
        }
    }

    /**
     * Executes a DuckDB query on multiple Parquet files.
     */
    private List<Map<String, Object>> queryParquetFiles(List<Path> files) throws Exception {
        if (!duckDbDriverLoaded) {
            throw new IllegalStateException("DuckDB driver not available");
        }

        // Build file list for read_parquet
        StringBuilder fileList = new StringBuilder();
        for (int i = 0; i < files.size(); i++) {
            if (i > 0) fileList.append(", ");
            String path = files.get(i).toAbsolutePath().toString().replace("\\", "/");
            fileList.append("'").append(path).append("'");
        }

        // union_by_name=true allows merging Parquet files with different schemas
        // (e.g., old files without avg_entropy, new files with it)
        String sql = String.format(
            "SELECT * FROM read_parquet([%s], union_by_name=true) ORDER BY tick",
            fileList
        );

        List<Map<String, Object>> result = new ArrayList<>();

        try (Connection conn = DriverManager.getConnection("jdbc:duckdb:");
             Statement stmt = conn.createStatement();
             ResultSet rs = stmt.executeQuery(sql)) {

            ResultSetMetaData meta = rs.getMetaData();
            int columnCount = meta.getColumnCount();
            String[] columnNames = new String[columnCount];
            for (int i = 0; i < columnCount; i++) {
                columnNames[i] = meta.getColumnName(i + 1);
            }

            while (rs.next()) {
                Map<String, Object> row = new LinkedHashMap<>(); // Preserve column order
                for (int i = 0; i < columnCount; i++) {
                    Object value = rs.getObject(i + 1);
                    // Handle BigInteger/Long for JSON compatibility
                    if (value instanceof java.math.BigInteger) {
                        value = ((java.math.BigInteger) value).longValue();
                    }
                    row.put(columnNames[i], value);
                }
                result.add(row);
            }
        }

        return result;
    }

    /**
     * Streams a merged Parquet file for client-side DuckDB WASM queries.
     * <p>
     * Route: GET /parquet?metric=...&amp;runId=...&amp;lod=...
     * <p>
     * This endpoint merges all Parquet files for a metric/LOD into a single file
     * and streams it to the client. The client can then use DuckDB WASM to run
     * arbitrary queries locally, including the generated SQL from the manifest.
     * <p>
     * The response is cacheable - set appropriate Cache-Control headers.
     *
     * @param ctx Javalin request context
     */
    @OpenApi(
        path = "parquet",
        methods = {HttpMethod.GET},
        summary = "Get merged Parquet file",
        description = "Merges all Parquet files for a metric/LOD into a single file and streams it. "
            + "Use this with client-side DuckDB WASM for flexible local queries. "
            + "The response is cacheable.",
        tags = {"analyzer / analytics"},
        queryParams = {
            @OpenApiParam(
                name = "metric",
                description = "Metric identifier (e.g., 'vital_stats')",
                required = true,
                example = "vital_stats"
            ),
            @OpenApiParam(
                name = "runId",
                description = "Simulation run ID. Optional - defaults to latest run.",
                required = false,
                example = "20251201-143025-550e8400-e29b-41d4-a716-446655440000"
            ),
            @OpenApiParam(
                name = "lod",
                description = "LOD level (e.g., 'lod0', 'lod1'). Default: lod0.",
                required = false,
                example = "lod0"
            )
        },
        responses = {
            @OpenApiResponse(
                status = "200", 
                description = "Merged Parquet file stream",
                content = @OpenApiContent(mimeType = "application/octet-stream")
            ),
            @OpenApiResponse(
                status = "400", 
                description = "Bad request (missing metric parameter)",
                content = @OpenApiContent(from = ErrorResponseDto.class)
            ),
            @OpenApiResponse(
                status = "404",
                description = "No data found for metric",
                content = @OpenApiContent(from = ErrorResponseDto.class)
            ),
            @OpenApiResponse(
                status = "500", 
                description = "Failed to merge Parquet files",
                content = @OpenApiContent(from = ErrorResponseDto.class)
            )
        }
    )
    private void getParquet(Context ctx) {
        String metric = ctx.queryParam("metric");
        String lod = ctx.queryParam("lod");

        if (metric == null || metric.isBlank()) {
            ctx.status(400).result("Missing metric parameter");
            return;
        }

        // Resolve runId (optional - defaults to latest)
        String runId;
        try {
            runId = resolveRunId(ctx);
        } catch (IllegalStateException e) {
            ctx.status(404).result("No simulation runs available");
            return;
        }
        
        // Resolve storage metric ID (may differ from manifest entry ID for merged plugins)
        String storageMetric = resolveStorageMetric(runId, metric);

        // Auto-select LOD if not specified (prefer higher LOD with fewer files for faster loading)
        if (lod == null || lod.isBlank()) {
            lod = autoSelectLod(runId, storageMetric);
            if (lod == null) {
                lod = "lod0"; // Fallback
            }
        }

        try {
            long startTime = System.currentTimeMillis();

            // 1. List all Parquet files for this metric/LOD
            String prefix = storageMetric + "/" + lod + "/";
            List<String> files = storage.listAnalyticsFiles(runId, prefix);
            List<String> parquetFiles = files.stream()
                .filter(f -> f.endsWith(".parquet"))
                .toList();

            if (parquetFiles.isEmpty()) {
                ctx.status(404).result("No data found for metric: " + metric);
                return;
            }

            // 2. Copy files from storage to temp directory
            Path tempDir = Files.createTempDirectory("analytics-parquet-");
            List<Path> tempFiles = new ArrayList<>();
            Path mergedFile = null;
            
            try {
                for (String file : parquetFiles) {
                    Path tempFile = tempDir.resolve(file.replace("/", "_"));
                    try (InputStream in = storage.openAnalyticsInputStream(runId, file);
                         OutputStream out = Files.newOutputStream(tempFile)) {
                        long bytesWritten = in.transferTo(out);
                        if (bytesWritten == 0) {
                            log.debug("Skipping empty Parquet file: {}", file);
                            Files.deleteIfExists(tempFile);
                            continue;
                        }
                    }
                    
                    // Validate file is readable (catches files still being written)
                    try {
                        validateParquetFile(tempFile);
                        tempFiles.add(tempFile);
                    } catch (Exception e) {
                        log.debug("Skipping unreadable Parquet file (may still be written): {}", file);
                        Files.deleteIfExists(tempFile);
                    }
                }
                
                if (tempFiles.isEmpty()) {
                    ctx.status(404).result("All Parquet files are empty for metric: " + metric);
                    return;
                }

                // 3. Merge Parquet files using DuckDB
                mergedFile = tempDir.resolve("merged.parquet");
                mergeParquetFiles(tempFiles, mergedFile);
                
                // 3a. Validate merged file is readable (catches race conditions with incomplete writes)
                long rowCount = validateParquetFile(mergedFile);
                if (rowCount == 0) {
                    ctx.status(404).result("No data available yet for metric: " + metric);
                    return;
                }

                long duration = System.currentTimeMillis() - startTime;
                log.debug("Merged {}/{}/{}: {} files into {} bytes ({} rows) in {}ms", 
                    runId, metric, lod, parquetFiles.size(), Files.size(mergedFile), rowCount, duration);

                // 4. Read merged file to memory and send to client
                // Note: Cannot use InputStream streaming because Javalin's ctx.result(InputStream)
                // is asynchronous, and the finally block would delete the file before streaming completes.
                ctx.contentType("application/octet-stream");
                ctx.header("Content-Disposition", "attachment; filename=\"" + metric + "_" + lod + ".parquet\"");
                // Short cache to allow quick refresh while simulation is running
                // Browser will revalidate after 5 seconds
                ctx.header("Cache-Control", "max-age=5");
                
                // Expose metadata for client-side debugging
                ctx.header("X-LOD-Level", lod);
                ctx.header("X-File-Count", String.valueOf(tempFiles.size()));
                ctx.header("X-Row-Count", String.valueOf(rowCount));
                ctx.header("X-Process-Time-Ms", String.valueOf(duration));
                
                byte[] mergedBytes = Files.readAllBytes(mergedFile);
                ctx.result(mergedBytes);

            } finally {
                // 5. Cleanup temp directory recursively
                // Must delete files before directory, sorted in reverse order (deepest first)
                try (var paths = Files.walk(tempDir)) {
                    paths.sorted(java.util.Comparator.reverseOrder())
                         .forEach(path -> {
                             try {
                                 Files.deleteIfExists(path);
                             } catch (IOException e) {
                                 log.debug("Failed to delete: {}", path);
                             }
                         });
                } catch (IOException e) {
                    log.debug("Failed to cleanup temp directory: {}", tempDir);
                }
            }

        } catch (Exception e) {
            log.error("Failed to get Parquet for {}/{}/{}", metric, lod, ctx.queryParam("runId"), e);
            ctx.status(500).result("Failed to merge Parquet files: " + e.getMessage());
        }
    }
    
    /**
     * Merges multiple Parquet files into one using DuckDB.
     *
     * @param inputFiles List of input Parquet files
     * @param outputFile Output merged Parquet file
     * @throws Exception if merge fails
     */
    private void mergeParquetFiles(List<Path> inputFiles, Path outputFile) throws Exception {
        if (!duckDbDriverLoaded) {
            throw new IllegalStateException("DuckDB driver not available");
        }

        // Build file list for read_parquet
        StringBuilder fileList = new StringBuilder();
        for (int i = 0; i < inputFiles.size(); i++) {
            if (i > 0) fileList.append(", ");
            String path = inputFiles.get(i).toAbsolutePath().toString().replace("\\", "/");
            fileList.append("'").append(path).append("'");
        }

        String outputPath = outputFile.toAbsolutePath().toString().replace("\\", "/");
        // union_by_name=true allows merging Parquet files with different schemas
        // (e.g., old files without avg_entropy, new files with it)
        String sql = String.format(
            "COPY (SELECT * FROM read_parquet([%s], union_by_name=true) ORDER BY tick) TO '%s' (FORMAT PARQUET, CODEC 'ZSTD')",
            fileList, outputPath
        );

        try (Connection conn = DriverManager.getConnection("jdbc:duckdb:");
             Statement stmt = conn.createStatement()) {
            stmt.execute(sql);
        }
    }
    
    /**
     * Validates that a Parquet file is readable and returns its row count.
     * This catches race conditions where files are still being written.
     *
     * @param parquetFile Path to the Parquet file
     * @return Number of rows in the file
     * @throws Exception if file is not readable
     */
    private long validateParquetFile(Path parquetFile) throws Exception {
        if (!duckDbDriverLoaded) {
            throw new IllegalStateException("DuckDB driver not available");
        }
        
        String path = parquetFile.toAbsolutePath().toString().replace("\\", "/");
        String sql = String.format("SELECT COUNT(*) FROM read_parquet('%s')", path);
        
        try (Connection conn = DriverManager.getConnection("jdbc:duckdb:");
             Statement stmt = conn.createStatement();
             ResultSet rs = stmt.executeQuery(sql)) {
            if (rs.next()) {
                return rs.getLong(1);
            }
            return 0;
        }
    }

    /**
     * Returns the aggregated manifest for all analytics plugins.
     * <p>
     * Route: GET /manifest?runId=...
     * <p>
     * The manifest is cached for a configurable TTL to reduce storage reads.
     *
     * @param ctx Javalin request context
     */
    @OpenApi(
        path = "manifest",
        methods = {HttpMethod.GET},
        summary = "Get analytics manifest",
        description = "Returns an aggregated manifest describing all available analytics metrics "
            + "for a simulation run. The manifest includes metadata from each plugin (name, "
            + "description, data sources, visualization hints). Response is cached.",
        tags = {"analyzer / analytics"},
        queryParams = {
            @OpenApiParam(
                name = "runId", 
                description = "Simulation run ID. Optional - defaults to latest run.", 
                required = false,
                example = "20251201-143025-550e8400-e29b-41d4-a716-446655440000"
            )
        },
        responses = {
            @OpenApiResponse(
                status = "200", 
                description = "Aggregated manifest with all available metrics",
                content = @OpenApiContent(
                    from = Map.class,
                    example = """
                        {
                          "metrics": [
                            {
                              "id": "population",
                              "name": "Population Overview",
                              "description": "Overview of alive organisms, total deaths, and average energy over time.",
                              "dataSources": {
                                "lod0": "population/lod0/**/*.parquet"
                              },
                              "visualization": {
                                "type": "line-chart",
                                "config": {
                                  "x": "tick",
                                  "y": ["alive_count", "total_dead"],
                                  "y2": ["avg_energy"]
                                }
                              }
                            }
                          ]
                        }
                        """
                )
            ),
            @OpenApiResponse(
                status = "400", 
                description = "Bad request (missing runId)", 
                content = @OpenApiContent(from = ErrorResponseDto.class)
            ),
            @OpenApiResponse(
                status = "500", 
                description = "Internal server error", 
                content = @OpenApiContent(from = ErrorResponseDto.class)
            )
        }
    )
    private void getManifest(Context ctx) {
        // Resolve runId (optional - defaults to latest)
        String runId;
        try {
            runId = resolveRunId(ctx);
        } catch (IllegalStateException e) {
            ctx.status(404).result("No simulation runs available");
            return;
        }

        // Check Cache
        CacheEntry entry = manifestCache.get(runId);
        if (entry != null && (System.currentTimeMillis() - entry.timestamp < cacheTtlMs)) {
            ctx.contentType("application/json").result(entry.json);
            return;
        }

        // Rebuild Manifest
        try {
            List<String> files = storage.listAnalyticsFiles(runId, "");
            List<ManifestEntry> entries = new ArrayList<>();

            for (String file : files) {
                if (file.endsWith("metadata.json")) {
                    try (InputStream in = storage.openAnalyticsInputStream(runId, file)) {
                        String json = new String(in.readAllBytes(), StandardCharsets.UTF_8);
                        ManifestEntry manifestEntry = gson.fromJson(json, ManifestEntry.class);
                        entries.add(manifestEntry);
                    } catch (Exception e) {
                        log.warn("Failed to read/parse metadata file: {}", file, e);
                    }
                }
            }

            Map<String, Object> response = Map.of("metrics", entries);
            String responseJson = gson.toJson(response);
            
            // Update Cache
            manifestCache.put(runId, new CacheEntry(System.currentTimeMillis(), responseJson));
            
            ctx.json(response);
            
        } catch (Exception e) {
            log.error("Failed to aggregate manifest for run {}", runId, e);
            ctx.status(500).result("Failed to generate manifest");
        }
    }

    /**
     * Resolves the storage metric ID for a given manifest entry ID.
     * <p>
     * When a single plugin produces multiple manifest entries, they share the same
     * underlying Parquet data stored under a common {@code storageMetricId}. This
     * method reads the entry's {@code metadata.json} and returns the storage metric ID
     * if set, otherwise falls back to using the entry ID directly.
     *
     * @param runId  The run identifier
     * @param metric The manifest entry ID (e.g., "genome_diversity")
     * @return The storage metric ID for locating Parquet files (e.g., "genome")
     */
    private String resolveStorageMetric(String runId, String metric) {
        try (InputStream in = storage.openAnalyticsInputStream(runId, metric + "/metadata.json")) {
            String json = new String(in.readAllBytes(), StandardCharsets.UTF_8);
            ManifestEntry entry = gson.fromJson(json, ManifestEntry.class);
            if (entry != null && entry.storageMetricId != null && !entry.storageMetricId.isBlank()) {
                return entry.storageMetricId;
            }
        } catch (Exception e) {
            // No metadata or parse error - use metric as-is
        }
        return metric;
    }
}
