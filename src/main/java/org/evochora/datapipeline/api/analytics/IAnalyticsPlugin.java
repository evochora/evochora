package org.evochora.datapipeline.api.analytics;

import java.util.Collections;
import java.util.List;

import org.evochora.datapipeline.api.contracts.TickData;
import org.evochora.datapipeline.api.memory.IMemoryEstimatable;
import org.evochora.datapipeline.api.memory.MemoryEstimate;
import org.evochora.datapipeline.api.memory.SimulationParameters;

import com.typesafe.config.Config;

/**
 * Interface for Analytics Plugins.
 * <p>
 * Plugins define WHAT data to extract, while the indexer handles HOW to write it
 * (DuckDB, Parquet, LOD generation, storage). This separation keeps plugins simple
 * and focuses them on domain logic.
 * <p>
 * <strong>Minimal Plugin Implementation:</strong>
 * <pre>{@code
 * public class MyMetricsPlugin extends AbstractAnalyticsPlugin {
 *     @Override
 *     public ParquetSchema getSchema() {
 *         return ParquetSchema.builder()
 *             .column("tick", ColumnType.BIGINT)
 *             .column("value", ColumnType.DOUBLE)
 *             .build();
 *     }
 *
 *     @Override
 *     public List<Object[]> extractRows(TickData tick) {
 *         return List.of(new Object[] { tick.getTickNumber(), computeValue(tick) });
 *     }
 *
 *     @Override
 *     public ManifestEntry getManifestEntry() { ... }
 * }
 * }</pre>
 * <p>
 * Instances are created per-service, so they do NOT need to be thread-safe.
 */
public interface IAnalyticsPlugin extends IMemoryEstimatable {
    
    /**
     * Configure the plugin from HOCON.
     * <p>
     * Standard options read by AbstractAnalyticsPlugin:
     * <ul>
     *   <li>{@code metricId} - Unique identifier for the metric (required)</li>
     *   <li>{@code samplingInterval} - Process every Nth tick (default: 1)</li>
     * </ul>
     *
     * @param config The plugin-specific configuration object
     */
    void configure(Config config);

    /**
     * Initialize plugin for a specific run.
     * <p>
     * Called once per run (or per service start). Use for any per-run setup.
     *
     * @param context Provides access to metadata and run information
     */
    void initialize(IAnalyticsContext context);
    
    /**
     * Returns the Parquet schema for this plugin's output.
     * <p>
     * The indexer uses this schema to:
     * <ol>
     *   <li>Create the DuckDB table</li>
     *   <li>Bind values from {@link #extractRows(TickData)}</li>
     *   <li>Export to Parquet format</li>
     * </ol>
     * <p>
     * <strong>Important:</strong> The schema must match the row arrays returned by
     * {@link #extractRows(TickData)} in both column count and type order.
     *
     * @return The Parquet schema definition
     */
    ParquetSchema getSchema();
    
    /**
     * Extracts row data from a single tick.
     * <p>
     * The indexer calls this for each tick (after sampling), collects the rows,
     * and handles all I/O (DuckDB insertion, Parquet export, storage upload).
     * <p>
     * Each {@code Object[]} in the returned list represents one row. The array
     * elements must match the schema column types:
     * <ul>
     *   <li>{@link ColumnType#BIGINT} → {@code Long}</li>
     *   <li>{@link ColumnType#INTEGER} → {@code Integer}</li>
     *   <li>{@link ColumnType#DOUBLE} → {@code Double}</li>
     *   <li>{@link ColumnType#VARCHAR} → {@code String}</li>
     *   <li>{@link ColumnType#BOOLEAN} → {@code Boolean}</li>
     * </ul>
     * <p>
     * <strong>Multiple Rows:</strong> Return multiple rows if a single tick produces
     * multiple data points (e.g., per-species breakdown).
     * <p>
     * <strong>No Data:</strong> Return empty list if this tick should be skipped
     * (beyond normal sampling).
     *
     * @param tick The tick data to process
     * @return List of rows (each row is Object[] matching schema), or empty list
     */
    List<Object[]> extractRows(TickData tick);

    /**
     * Called when indexer shuts down or finishes a run.
     * <p>
     * Use for any cleanup. Note: The indexer handles Parquet flushing automatically.
     */
    void onFinish();

    /**
     * Returns the manifest entry describing the metric generated by this plugin.
     * <p>
     * The indexer writes this to {@code {metric_id}/metadata.json} (idempotently).
     * The manifest tells the frontend how to discover and visualize this metric.
     *
     * @return The manifest entry, or null if plugin produces no visible metric
     */
    ManifestEntry getManifestEntry();
    
    /**
     * Returns the unique metric identifier for this plugin.
     * <p>
     * Used by the indexer for:
     * <ul>
     *   <li>Storage paths: {@code {runId}/analytics/{metricId}/...}</li>
     *   <li>Logging and error messages</li>
     *   <li>Manifest aggregation</li>
     * </ul>
     *
     * @return The metric ID (e.g., "population", "energy", "spatial")
     */
    String getMetricId();
    
    /**
     * Returns the configured sampling interval.
     * <p>
     * The indexer uses this to skip ticks: only process every Nth tick.
     *
     * @return Sampling interval (1 = every tick, 10 = every 10th tick)
     */
    int getSamplingInterval();
    
    /**
     * Returns the LOD (Level of Detail) factor.
     * <p>
     * Each higher LOD level samples at {@code lodFactor^level} times the base interval.
     * Example with lodFactor=10 and samplingInterval=1:
     * <ul>
     *   <li>lod0: every tick (interval=1)</li>
     *   <li>lod1: every 10th tick (interval=10)</li>
     *   <li>lod2: every 100th tick (interval=100)</li>
     * </ul>
     *
     * @return LOD factor (default: 10)
     */
    int getLodFactor();
    
    /**
     * Returns the number of LOD levels to generate.
     * <p>
     * The indexer generates separate Parquet files for each LOD level:
     * <ul>
     *   <li>lodLevels=1: only lod0 (full resolution)</li>
     *   <li>lodLevels=2: lod0 + lod1 (10x downsampled)</li>
     *   <li>lodLevels=3: lod0 + lod1 + lod2 (100x downsampled)</li>
     * </ul>
     *
     * @return Number of LOD levels (default: 1)
     */
    int getLodLevels();
    
    /**
     * Returns the query specification for query-time transformations.
     * <p>
     * The QuerySpec defines:
     * <ul>
     *   <li>Base columns (stored in Parquet)</li>
     *   <li>Computed columns (calculated via SQL window functions)</li>
     *   <li>Output columns (what the query returns)</li>
     * </ul>
     * <p>
     * The controller uses this to generate SQL for both server-side queries
     * and client-side DuckDB WASM.
     * <p>
     * Default implementation returns a passthrough spec (no transformations).
     *
     * @return Query specification, or null for passthrough
     */
    default QuerySpec getQuerySpec() {
        return QuerySpec.passthrough(getSchema());
    }
    
    /**
     * Returns the visualization specification for the frontend.
     * <p>
     * Defines how the data should be rendered (chart type, axes, styling).
     * <p>
     * Default implementation returns a simple line chart with tick on X axis.
     *
     * @return Visualizer specification
     */
    default VisualizerSpec getVisualizerSpec() {
        return VisualizerSpec.lineChart("tick");
    }

    /**
     * Estimates the worst-case heap memory usage for this plugin's internal state.
     * <p>
     * The default implementation returns an empty list, assuming no significant
     * internal state. Stateful plugins (e.g., those tracking lineage or history)
     * MUST override this method to provide an accurate estimate based on the
     * provided simulation parameters.
     *
     * @param params Parameters of the simulation (e.g., max organisms, world size).
     * @return A list of memory estimates for different components of the plugin's state.
     *         Returns an empty list by default if the plugin is stateless.
     */
    @Override
    default List<MemoryEstimate> estimateWorstCaseMemory(SimulationParameters params) {
        return Collections.emptyList();
    }
}
